import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from imblearn.over_sampling import SMOTE  # imblearn library can be installed using pip install imblearn
from sklearn.ensemble import RandomForestClassifier
from imblearn.pipeline import Pipeline
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import recall_score, classification_report, confusion_matrix

pip install xgboost

# Read the CSV file
df = pd.read_csv(r"C:\Users\ragha\Desktop\Attack_Dataset.csv")

# Drop rows with missing 'class' values
df = df.dropna(subset=['class'])

# Display network traffic data
print('====Network Traffic Data====')
df_data = df.drop(['class'], axis=1)
print(df_data)

# Display attack types and their counts
print('====Attack Types====')
attack_counts = df['class'].value_counts()
print(attack_counts)

pd.set_option('display.max_columns', None) # to make sure you can see all the columns in output window
print(df.head())
print(df.shape)
print(df.info())
print(df.describe())

# Converting Categorical features into Numerical features
df['class'] = df['class'].map({'normal':0, 'anomaly':1})
print(df.info())

#DataFrame is named 'df'
categorical_features = ['service', 'flag', 'protocol_type']
final_data = pd.get_dummies(df, columns=categorical_features, drop_first=True)  # Use drop_first=True to avoid multicollinearity
print(final_data.info())
print(final_data.head(2))
#DataFrame is named 'df'
print(df.dtypes)
#DataFrame is named 'df'
df.info()

# Dividing dataset into label and feature sets
X = final_data.drop('class', axis = 1) # Features
Y = final_data['class'] # Labels
print(type(X))
print(type(Y))
print(X.shape)
print(Y.shape)

import matplotlib.pyplot as plt
import seaborn as sns

# Visualize the distribution of the target class
plt.figure(figsize=(6, 4))
sns.countplot(data=final_data, x='class')
plt.title('Distribution of Target Class')
plt.xlabel('Class')
plt.ylabel('Count')
plt.show()

# Visualize the correlation matrix of features
plt.figure(figsize=(10, 8))
correlation_matrix = X.corr()
sns.heatmap(correlation_matrix, cmap='coolwarm', annot=False)
plt.title('Correlation Matrix of Features')
plt.show()

# Visualize a pair plot of selected features
sns.pairplot(data=final_data, vars=['src_bytes', 'dst_bytes', 'logged_in'], hue='class')
plt.suptitle('Pair Plot of Selected Features')
plt.show()

# Visualize box plots of some features by class
features_to_plot = ['src_bytes', 'dst_bytes']
for feature in features_to_plot:
    plt.figure(figsize=(6, 4))
    sns.boxplot(data=final_data, x='class', y=feature)
    plt.title(f'Box Plot of {feature} by Class')
    plt.show()

# Normalizing numerical features so that each feature has mean 0 and variance 1
feature_scaler = StandardScaler()
X_scaled = feature_scaler.fit_transform(X)

# Implementing Random Forest Classifier
# Tuning the random forest parameter 'n_estimators' and implementing cross-validation using Grid Search
model = Pipeline([
        ('balancing', SMOTE(random_state = 101)),
        ('classification', RandomForestClassifier(criterion='entropy', max_features='sqrt', random_state=1) )
    ])
grid_param = {'classification__n_estimators': [10,20,30,40,50,100]}

gd_sr = GridSearchCV(estimator=model, param_grid=grid_param, scoring='recall', cv=5)

"""
In the above GridSearchCV(), scoring parameter should be set as follows:
scoring = 'accuracy' when you want to maximize prediction accuracy
scoring = 'recall' when you want to minimize false negatives
scoring = 'precision' when you want to minimize false positives
scoring = 'f1' when you want to balance false positives and false negatives (place equal emphasis on minimizing both)
"""

gd_sr.fit(X_scaled, Y)

best_parameters = gd_sr.best_params_
print(best_parameters)

best_result = gd_sr.best_score_ # Mean cross-validated score of the best_estimator
print(best_result)

featimp = pd.Series(gd_sr.best_estimator_.named_steps["classification"].feature_importances_, index=list(X)).sort_values(ascending=False) # Getting feature importances list for the best model
print(featimp)

# Selecting features with higher sifnificance and redefining feature set
X_ = final_data[['diff_srv_rate','su_attempted','rerror_rate','logged_in']]

feature_scaler = StandardScaler()
X_scaled_ = feature_scaler.fit_transform(X_)

#Tuning the random forest parameter 'n_estimators' and implementing cross-validation using Grid Search
model = Pipeline([
        ('balancing', SMOTE(random_state = 101)),
        ('classification', RandomForestClassifier(criterion='entropy', max_features='sqrt', random_state=1) )
    ])
grid_param = {'classification__n_estimators': [10,20,30,40,50,100]}

gd_sr = GridSearchCV(estimator=model, param_grid=grid_param, scoring='recall', cv=5)

"""
In the above GridSearchCV(), scoring parameter should be set as follows:
scoring = 'accuracy' when you want to maximize prediction accuracy
scoring = 'recall' when you want to minimize false negatives
scoring = 'precision' when you want to minimize false positives
scoring = 'f1' when you want to balance false positives and false negatives (place equal emphasis on minimizing both)
"""

gd_sr.fit(X_scaled_, Y)

best_parameters = gd_sr.best_params_
print(best_parameters)

best_result = gd_sr.best_score_ # Mean cross-validated score of the best_estimator
print(best_result)

# Implementing Support Vector Classifier
# Tuning the kernel parameter and implementing cross-validation using Grid Search
model = Pipeline([
        ('balancing', SMOTE(random_state = 101)),
        ('classification', SVC(random_state=1) )
    ])
grid_param = {'classification__kernel': ['linear','poly','rbf','sigmoid'], 'classification__C': [0.1,1,10]}

gd_sr = GridSearchCV(estimator=model, param_grid=grid_param, scoring='recall', cv=5)

"""
In the above GridSearchCV(), scoring parameter should be set as follows:
scoring = 'accuracy' when you want to maximize prediction accuracy
scoring = 'recall' when you want to minimize false negatives
scoring = 'precision' when you want to minimize false positives
scoring = 'f1' when you want to balance false positives and false negatives (place equal emphasis on minimizing both)
"""

gd_sr.fit(X_scaled, Y)

best_parameters = gd_sr.best_params_
print(best_parameters)

best_result = gd_sr.best_score_ # Mean cross-validated score of the best_estimator
print(best_result)

# Preparing a model for recall score using XGBoost
model = Pipeline([
    ('balancing', SMOTE(random_state=101)),
    ('classification', XGBClassifier(objective='binary:logistic', eval_metric='logloss', use_label_encoder=False))
])
grid_param = {'classification__n_estimators': [10, 40, 70, 100, 130], 'classification__learning_rate': [0.01, 0.1, 0.02, 0.2, 0.03, 0.3, 0.04, 0.4, 0.05, 0.5]}

gd_sr = GridSearchCV(estimator=model, param_grid=grid_param, scoring='recall', cv=5)
gd_sr.fit(X_scaled, Y)

# Best Parameters with Recall Score
best_parameters = gd_sr.best_params_
print(best_parameters)

best_result = gd_sr.best_score_
print(best_result)

# Preparing a model for precision score using XGBoost
model = Pipeline([
    ('balancing', SMOTE(random_state=101)),
    ('classification', XGBClassifier(objective='binary:logistic', eval_metric='logloss', use_label_encoder=False))
])
grid_param_pr = {'classification__n_estimators': [40], 'classification__learning_rate': [0.1]}

gd_sr_pr = GridSearchCV(estimator=model, param_grid=grid_param_pr, scoring='precision', cv=5)
gd_sr_pr.fit(X_scaled, Y)

# Best Precision Score
best_result_pr = gd_sr_pr.best_score_
print(best_result_pr)
